# utils/ultrafast_rag_optimized.py

import os
import re
import time
import hashlib
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
from functools import lru_cache
import concurrent.futures
import threading

import numpy as np
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Import local
from utils.optimized_vector_store import OptimizedVectorStore

# Import condicional para Ollama
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    try:
        from langchain_community.llms import Ollama as OllamaLLM
    except ImportError:
        try:
            from langchain.llms import Ollama as OllamaLLM
        except ImportError:
            class OllamaLLM:
                def __init__(self, *args, **kwargs):
                    print("‚ö†Ô∏è Usando OllamaLLM mock")
                def invoke(self, prompt): return "Resposta mock - Ollama n√£o dispon√≠vel"
                def __call__(self, prompt): return "Resposta mock - Ollama n√£o dispon√≠vel"


@dataclass
class UltraFastRAGConfig:
    model_name: str = "gemma:2b"  # ‚ö° SEU MODELO
    temperature: float = 0.0
    chunk_size: int = 2000  # ‚ö° AUMENTADO de 800
    chunk_overlap: int = 400  # ‚ö° AUMENTADO de 200
    top_k: int = 8  # ‚ö° AUMENTADO de 7
    max_chunks: int = 1000  # ‚ö° AUMENTADO de 50 (MUITO IMPORTANTE!)
    data_dir: str = "data"
    use_ollama_embeddings: bool = True
    enable_conversational: bool = True
    max_context_length: int = 20000  # ‚ö° AUMENTADO de 8192
    num_predict: int = 1500  # ‚ö° AUMENTADO de 1024
    enable_cache: bool = True
    cache_ttl: int = 3600 
    min_similarity_score: float = 0.001  # ‚ö° REDUZIDO de 0.15
    enable_parallel_search: bool = True
    enable_preprocessing: bool = True


class UltraFastRAG:
    def __init__(self, config: Optional[UltraFastRAGConfig] = None):
        self.config = config or UltraFastRAGConfig()
        self.llm = None
        self.vector_store = None
        self.documents = []
        self.is_initialized = False
        self.conversational_handler = None
        
        # Cache em mem√≥ria para respostas
        self.response_cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}
        
        # Lock para thread safety
        self.cache_lock = threading.Lock()
        
        # Caminhos configur√°veis
        self.data_path = os.getenv("DADOS_ANONIMOS", 
                                  os.getenv("PASTA_DESTINO", 
                                           self.config.data_dir))
        self.cache_path = os.path.join(os.path.dirname(self.data_path), ".rag_cache")
        os.makedirs(self.cache_path, exist_ok=True)
        
        # Splitter otimizado para chunks menores
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ". ", "! ", "? ", " "]
        )
        
        # Padr√µes de preprocessamento compilados
        self._compile_preprocessing_patterns()
    
    def _compile_preprocessing_patterns(self):
        """Compila padr√µes regex para preprocessamento r√°pido"""
        self.cleanup_patterns = [
            (re.compile(r'\s+'), ' '),  # M√∫ltiplos espa√ßos
            (re.compile(r'\n+'), '\n'),  # M√∫ltiplas quebras
            (re.compile(r'[^\w\s\.\?\!\,\:\;\-\(\)]', re.UNICODE), ''),  # Caracteres especiais
        ]
        
        self.query_patterns = [
            (re.compile(r'\b(qual|quais|como|quando|onde|por que|porque)\b', re.IGNORECASE), ''),
            (re.compile(r'\b(me|nos|lhe|te)\s+(ajude|ajudar|diga|dizer|fale|falar)\b', re.IGNORECASE), ''),
            (re.compile(r'\b(por favor|obrigad[oa]|valeu)\b', re.IGNORECASE), ''),
        ]
    
    @lru_cache(maxsize=1000)
    def _preprocess_query(self, query: str) -> str:
        """Preprocessa query para melhor busca (com cache)"""
        if not self.config.enable_preprocessing:
            return query
        
        # Remove palavras de cortesia e melhora a query
        processed = query.strip().lower()
        
        for pattern, replacement in self.query_patterns:
            processed = pattern.sub(replacement, processed)
        
        # Remove espa√ßos extras
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        # Mant√©m palavras-chave importantes
        keywords = [word for word in processed.split() 
                   if len(word) > 2 and word not in ['para', 'com', 'por', 'sobre']]
        
        return ' '.join(keywords) if keywords else query
    
    def _get_cache_key(self, query: str, top_k: int) -> str:
        """Gera chave √∫nica para cache"""
        content = f"{query}_{top_k}_{self.config.model_name}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Busca resposta no cache"""
        if not self.config.enable_cache:
            return None
        
        with self.cache_lock:
            if cache_key in self.response_cache:
                cached_data, timestamp = self.response_cache[cache_key]
                if time.time() - timestamp < self.config.cache_ttl:
                    self.cache_stats["hits"] += 1
                    return cached_data
                else:
                    # Remove cache expirado
                    del self.response_cache[cache_key]
        
        self.cache_stats["misses"] += 1
        return None
    
    def _save_to_cache(self, cache_key: str, response: Dict[str, Any]) -> None:
        """Salva resposta no cache"""
        if not self.config.enable_cache:
            return
        
        with self.cache_lock:
            # Limita tamanho do cache
            if len(self.response_cache) > 500:
                # Remove entradas mais antigas
                oldest_key = min(self.response_cache.keys(), 
                               key=lambda k: self.response_cache[k][1])
                del self.response_cache[oldest_key]
            
            self.response_cache[cache_key] = (response, time.time())
    
    def initialize(self):
        """Inicializa o sistema RAG otimizado"""
        try:
            print("üöÄ Inicializando UltraFast RAG Otimizado...")
            
            # Inicializa LLM com configura√ß√µes para contexto completo
            try:
                self.llm = OllamaLLM(
                    model=self.config.model_name,
                    temperature=self.config.temperature,
                    num_predict=self.config.num_predict,  # ‚ö° Agora 1500
                    num_ctx=8192,  # ‚ö° Contexto m√°ximo
                    repeat_penalty=1.1,
                    top_k=40,  # ‚ö° Aumentado
                    top_p=0.9,
                    stop=[],  # ‚ö° Sem limita√ß√µes
                )
                
                # Teste r√°pido de conex√£o
                test_response = self.llm.invoke("OK")
                print(f"‚úÖ LLM {self.config.model_name} otimizado conectado")
                
            except Exception as e:
                print(f"‚ùå Erro na conex√£o LLM: {e}")
                return False
            
            self.is_initialized = True
            self._load_cache()
            
            # Integra sistema conversacional se habilitado
            if self.config.enable_conversational:
                try:
                    self._integrate_conversational()
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro na integra√ß√£o conversacional: {e}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico na inicializa√ß√£o: {e}")
            self.is_initialized = False
            return False

    
    def _integrate_conversational(self):
        """Integra sistema conversacional de forma otimizada"""
        try:
            import sys
            if 'adaptive_rag' in sys.modules:
                module = sys.modules['adaptive_rag']
                if hasattr(module, 'enhance_rag_with_conversation'):
                    enhance_rag_with_conversation = module.enhance_rag_with_conversation
                    enhance_rag_with_conversation(self)
                    print("‚úÖ Sistema conversacional integrado!")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao integrar conversacional: {e}")
    
    def _load_cache(self):
        """Carrega cache de forma otimizada"""
        try:
            self.vector_store = OptimizedVectorStore(
                os.path.join(self.cache_path, "optimized"),
                use_ollama=self.config.use_ollama_embeddings
            )
            
            if self.vector_store.load():
                try:
                    self.documents = []
                    for i, (doc_content, metadata) in enumerate(zip(self.vector_store.documents, self.vector_store.metadata)):
                        self.documents.append(Document(page_content=doc_content, metadata=metadata))
                    print(f"üì¶ Cache carregado: {len(self.documents)} documentos")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao processar cache: {e}")
                    self.documents = []
            else:
                self.documents = []
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar cache: {e}")
            self.vector_store = None
    
    def load_documents_from_directory(self):
        """Carrega documentos de forma otimizada"""
        if not os.path.exists(self.data_path):
            print(f"‚ö†Ô∏è Diret√≥rio n√£o encontrado: {self.data_path}")
            return 0
        
        print(f"üìÅ Carregando documentos otimizado de: {self.data_path}")
        
        documents = []
        try:
            for root, dirs, files in os.walk(self.data_path):
                for file in files:
                    if file.lower().endswith(('.txt', '.md')):
                        try:
                            filepath = os.path.join(root, file)
                            with open(filepath, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                            
                            if len(content) < 100:
                                continue
                            
                            # Parse de metadados otimizado
                            metadata = self._parse_legal_metadata_fast(content)
                            metadata['filename'] = file
                            metadata['source'] = filepath
                            
                            # Limpeza otimizada do conte√∫do
                            clean_content = self._clean_content_fast(content)
                            
                            # Chunking otimizado
                            chunks = self.text_splitter.split_text(clean_content)
                            for chunk in chunks:
                                if len(chunk.strip()) > 100:
                                    documents.append(Document(
                                        page_content=chunk.strip(),
                                        metadata=metadata.copy()
                                    ))
                            
                        except Exception as e:
                            continue
            
            if documents:
                print(f"üìÑ {len(documents)} chunks carregados (otimizado)")
                self.documents = documents
                self._create_vector_store()
            
            return len(documents)
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar documentos: {e}")
            return 0
    
    @lru_cache(maxsize=100)
    def _parse_legal_metadata_fast(self, content: str) -> Dict[str, str]:
        """Parse otimizado de metadados com cache"""
        metadata = {}
        
        # Parse mais direto com regex
        patterns = {
            'numero_processo': r'numero_processo[:\s]+["\']?([^"\'\n]+)["\']?',
            'agravante': r'agravante[:\s]+["\']?([^"\'\n]+)["\']?',
            'agravado': r'agravado[:\s]+["\']?([^"\'\n]+)["\']?',
            'valor_causa': r'valor_causa[:\s]+["\']?([^"\'\n]+)["\']?',
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                metadata[key] = match.group(1).strip()
        
        return metadata
    
    def _clean_content_fast(self, content: str) -> str:
        """Limpeza otimizada de conte√∫do"""
        # Remove se√ß√µes de metadados
        content = re.sub(r"### METADADOS DO PROCESSO.*?(?=\n###|\Z)", "", content, flags=re.DOTALL)
        content = re.sub(r"### PARTES ENVOLVIDAS.*?(?=\n###|\Z)", "", content, flags=re.DOTALL)
        
        # Limpeza b√°sica
        for pattern, replacement in self.cleanup_patterns:
            content = pattern.sub(replacement, content)
        
        return content.strip()
    
    def _create_vector_store(self):
        """Cria vector store otimizado"""
        if not self.documents:
            return
        
        try:
            print("üóÉÔ∏è Criando vector store otimizado...")
            
            self.vector_store = OptimizedVectorStore(
                os.path.join(self.cache_path, "optimized"),
                use_ollama=self.config.use_ollama_embeddings
            )
            
            # Limita documentos para performance
            max_docs = min(len(self.documents), self.config.max_chunks)
            self.vector_store.add_documents(self.documents[:max_docs], max_docs=max_docs)
            print("‚úÖ Vector store otimizado criado!")
            
        except Exception as e:
            print(f"‚ùå Erro ao criar vector store: {e}")
            self.vector_store = None
    
    def _search_documents_optimized(self, query: str, top_k: int) -> List[Document]:
        """Busca MUITO mais permissiva e abrangente"""
        if not self.vector_store:
            return []
        
        print(f"üîç Busca h√≠brida para: '{query}'")
        
        # === BUSCA 1: KEYWORDS ESPEC√çFICAS ===
        keyword_results = []
        
        # Keywords jur√≠dicas espec√≠ficas
        legal_keywords = [
            'argumento', 'argumenta', 'sustenta', 'alega', 'defesa', 
            'motiva√ß√£o', 'fundamento', 'raz√£o', 'motivo',
            'pad', 'processo administrativo', 'disciplinar', 'demiss√£o',
            'cerceamento', 'contradit√≥rio', 'ampla defesa',
            'tutela', 'liminar', 'urg√™ncia', 'recurso', 'agravo'
        ]
        
        # Identifica keywords relevantes
        query_lower = query.lower()
        relevant_keywords = []
        
        for keyword in legal_keywords:
            if keyword in query_lower:
                relevant_keywords.append(keyword)
        
        # Se n√£o achou keywords espec√≠ficas, usa palavras da query
        if not relevant_keywords:
            relevant_keywords = [word for word in query_lower.split() if len(word) > 3]
        
        print(f"üìù Buscando por: {relevant_keywords}")
        
        # Busca keywords nos documentos
        for doc in self.documents or []:
            content_lower = doc.page_content.lower()
            score = 0
            
            for keyword in relevant_keywords:
                count = content_lower.count(keyword.lower())
                score += count
            
            if score > 0:
                doc_copy = Document(
                    page_content=doc.page_content,
                    metadata={
                        **doc.metadata,
                        "similarity_score": min(score / 5, 1.0),
                        "search_type": "keyword"
                    }
                )
                keyword_results.append((score, doc_copy))
        
        keyword_results.sort(key=lambda x: x[0], reverse=True)
        keyword_docs = [doc for _, doc in keyword_results[:top_k]]
        
        print(f"üéØ Keywords encontraram: {len(keyword_docs)} docs")
        
        # === BUSCA 2: N√öMERO DE PROCESSO ===
        process_results = []
        process_pattern = r'\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4}'
        process_match = re.search(process_pattern, query)
        
        if process_match:
            process_number = process_match.group()
            print(f"üî¢ Processo espec√≠fico: {process_number}")
            
            for doc in self.documents or []:
                if process_number in doc.page_content:
                    doc_copy = Document(
                        page_content=doc.page_content,
                        metadata={
                            **doc.metadata,
                            "similarity_score": 0.95,
                            "search_type": "process"
                        }
                    )
                    process_results.append(doc_copy)
            
            print(f"üî¢ Processo encontrou: {len(process_results)} docs")
        
        # === BUSCA 3: SEM√ÇNTICA PERMISSIVA ===
        semantic_results = []
        try:
            semantic_results = self.vector_store.similarity_search(
                query, k=top_k, min_score=0.001  # MUITO permissivo
            )
            print(f"üß† Sem√¢ntica encontrou: {len(semantic_results)} docs")
        except Exception as e:
            print(f"‚ö†Ô∏è Busca sem√¢ntica falhou: {e}")
        
        # === COMBINA RESULTADOS ===
        all_results = []
        seen = set()
        
        # Prioriza: processo > keywords > sem√¢ntica
        for result_set in [process_results, keyword_docs, semantic_results]:
            for doc in result_set:
                doc_hash = hash(doc.page_content[:100])
                if doc_hash not in seen:
                    seen.add(doc_hash)
                    all_results.append(doc)
        
        final_results = all_results[:top_k]
        print(f"‚úÖ Total selecionado: {len(final_results)} documentos")
        
        return final_results
    
    def _create_optimized_context(self, docs: List[Document]) -> str:
        """Cria contexto SEM limita√ß√µes"""
        if not docs:
            return ""
        
        context_parts = []
        total_length = 0
        max_length = self.config.max_context_length  # Agora 20000
        
        for i, doc in enumerate(docs, 1):
            content = doc.page_content.strip()
            
            # ‚ö° REMOVIDO: Limita√ß√£o de 400 chars por documento
            # Agora usa documento COMPLETO
            
            # Metadados importantes
            metadata_info = []
            for key in ['numero_processo', 'agravante', 'agravado', 'assuntos', 'valor_causa']:
                if key in doc.metadata and doc.metadata[key]:
                    metadata_info.append(f"{key}: {doc.metadata[key]}")
            
            # Monta documento completo
            doc_text = f"DOCUMENTO {i}"
            if metadata_info:
                doc_text += f" (Metadados: {', '.join(metadata_info)})"
            doc_text += f":\n{content}"  # ‚ö° DOCUMENTO COMPLETO
            
            # S√≥ limita se n√£o couber no total
            if total_length + len(doc_text) > max_length:
                remaining = max_length - total_length - 500
                if remaining > 1000:
                    doc_text = f"DOCUMENTO {i} (PARCIAL):\n{content[:remaining]}..."
                    context_parts.append(doc_text)
                break
            
            context_parts.append(doc_text)
            total_length += len(doc_text)
        
        return "\n\n" + "="*50 + "\n\n".join(context_parts)
    
    def _create_optimized_prompt(self, question: str, context: str) -> str:
        """Prompt otimizado para encontrar conte√∫do espec√≠fico"""
        return f"""Voc√™ √© um assistente jur√≠dico especializado. Analise TODOS os documentos fornecidos e responda de forma DETALHADA.

    INSTRU√á√ïES CR√çTICAS:
    1. Use TODAS as informa√ß√µes relevantes dos documentos
    2. Para argumentos de defesa: extraia alega√ß√µes, sustenta√ß√µes espec√≠ficas
    3. Para motiva√ß√µes: identifique tipo de a√ß√£o, fundamentos, raz√µes
    4. Cite trechos espec√≠ficos quando relevante
    5. N√ÉO se limite aos metadados - use o CONTE√öDO COMPLETO
    6. Se houver m√∫ltiplas informa√ß√µes, inclua todas

    DOCUMENTOS COMPLETOS:
    {context}

    PERGUNTA: {question}

    RESPOSTA DETALHADA E FUNDAMENTADA:"""
    
    def query(self, question: str, top_k: Optional[int] = None) -> Dict[str, Any]:
        """Query otimizada com cache e paraleliza√ß√£o"""
        if not self.is_initialized or not self.vector_store:
            return {"error": "Sistema n√£o inicializado"}
        
        k = top_k or self.config.top_k
        
        # Verifica cache primeiro
        cache_key = self._get_cache_key(question, k)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            cached_result["from_cache"] = True
            return cached_result
        
        try:
            start_time = time.time()
            
            # Busca otimizada de documentos
            relevant_docs = self._search_documents_optimized(question, k)
            
            if not relevant_docs:
                result = {
                    "error": "Nenhum documento relevante encontrado",
                    "suggestion": "Tente reformular a pergunta",
                    "processing_time": time.time() - start_time
                }
                self._save_to_cache(cache_key, result)
                return result
            
            # Cria contexto otimizado
            context = self._create_optimized_context(relevant_docs)
            
            # Cria prompt otimizado
            prompt = self._create_optimized_prompt(question, context)
            
            # Chama LLM de forma otimizada
            if hasattr(self.llm, 'invoke'):
                answer = self.llm.invoke(prompt)
            else:
                answer = self.llm(prompt)
            
            result = {
                "answer": answer.strip(),
                "documents_found": len(relevant_docs),
                "processing_time": time.time() - start_time,
                "from_cache": False,
                "cache_stats": self.cache_stats.copy()
            }
            
            # Salva no cache
            self._save_to_cache(cache_key, result)
            
            return result
            
        except Exception as e:
            error_result = {"error": str(e), "processing_time": time.time() - start_time}
            return error_result
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas de performance"""
        return {
            "cache_stats": self.cache_stats.copy(),
            "cache_size": len(self.response_cache),
            "documents_loaded": len(self.documents) if self.documents else 0,
            "config": {
                "num_predict": self.config.num_predict,
                "max_context_length": self.config.max_context_length,
                "top_k": self.config.top_k,
                "chunk_size": self.config.chunk_size,
                "enable_cache": self.config.enable_cache,
                "enable_parallel_search": self.config.enable_parallel_search,
                "enable_preprocessing": self.config.enable_preprocessing
            }
        }
    
    def clear_cache(self):
        """Limpa cache de respostas"""
        with self.cache_lock:
            self.response_cache.clear()
            self.cache_stats = {"hits": 0, "misses": 0}
        print("üóëÔ∏è Cache limpo")


# =================================================================================
# INST√ÇNCIA GLOBAL E FUN√á√ïES DE INTERFACE OTIMIZADAS
# =================================================================================

# Inst√¢ncia global otimizada
optimized_rag_system = None

def init_optimized_rag():
    """Inicializa sistema RAG com configura√ß√µes corretas"""
    global optimized_rag_system
    
    config = UltraFastRAGConfig(
        model_name="gemma:2b",  # ‚ö° SEU MODELO
        enable_cache=True,
        enable_parallel_search=True,
        enable_preprocessing=True,
        num_predict=1500,  # ‚ö° Respostas completas
        max_context_length=20000,  # ‚ö° Contexto expandido
        top_k=8,  # ‚ö° Mais documentos
        min_similarity_score=0.001,  # ‚ö° Muito permissivo
        max_chunks=1000  # ‚ö° CR√çTICO: documentos suficientes
    )
    
    optimized_rag_system = UltraFastRAG(config)  # ‚ö° CLASSE CORRETA
    return optimized_rag_system.initialize()


def load_optimized_data():
    """Carrega dados de forma otimizada"""
    if optimized_rag_system:
        return optimized_rag_system.load_documents_from_directory()
    return 0

def query_optimized_rag(question: str, top_k: int = 3):
    """Interface otimizada para consultas"""
    if optimized_rag_system:
        return optimized_rag_system.query(question, top_k)
    return {"error": "Sistema n√£o inicializado"}

def get_optimized_performance_stats():
    """Estat√≠sticas de performance"""
    if optimized_rag_system:
        return optimized_rag_system.get_performance_stats()
    return {"error": "Sistema n√£o inicializado"}

def clear_optimized_cache():
    """Limpa cache do sistema otimizado"""
    if optimized_rag_system:
        optimized_rag_system.clear_cache()


# =================================================================================
# TESTE DE PERFORMANCE
# =================================================================================

def performance_comparison_test():
    """Teste de compara√ß√£o de performance"""
    print("üèÉ‚Äç‚ôÇÔ∏è TESTE DE PERFORMANCE - RAG OTIMIZADO")
    print("=" * 60)
    
    if not optimized_rag_system or not optimized_rag_system.is_initialized:
        print("‚ùå Sistema otimizado n√£o inicializado")
        return
    
    # Queries de teste
    test_queries = [
        "processo 1005888",
        "terapia ABA",
        "valor da causa",
        "agravante agravado",
        "SUS tratamento"
    ]
    
    print("üß™ Testando velocidade das consultas...")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- TESTE {i} ---")
        print(f"Query: {query}")
        
        start_time = time.time()
        result = optimized_rag_system.query(query)
        end_time = time.time()
        
        if "error" not in result:
            print(f"‚úÖ Resposta obtida")
            print(f"‚è±Ô∏è Tempo: {end_time - start_time:.2f}s")
            print(f"üìä Docs encontrados: {result.get('documents_found', 0)}")
            print(f"üíæ Do cache: {result.get('from_cache', False)}")
        else:
            print(f"‚ùå Erro: {result['error']}")
    
    # Estat√≠sticas finais
    print(f"\nüìà ESTAT√çSTICAS FINAIS:")
    stats = optimized_rag_system.get_performance_stats()
    print(f"Cache hits: {stats['cache_stats']['hits']}")
    print(f"Cache misses: {stats['cache_stats']['misses']}")
    if stats['cache_stats']['hits'] + stats['cache_stats']['misses'] > 0:
        hit_rate = stats['cache_stats']['hits'] / (stats['cache_stats']['hits'] + stats['cache_stats']['misses'])
        print(f"Taxa de acerto do cache: {hit_rate:.2%}")

if __name__ == "__main__":
    print("üöÄ SISTEMA RAG ULTRA OTIMIZADO")
    print("=" * 50)
    
    # Inicializa sistema otimizado
    if init_optimized_rag():
        print("‚úÖ Sistema otimizado inicializado")
        
        # Carrega dados
        docs_loaded = load_optimized_data()
        if docs_loaded > 0:
            print(f"‚úÖ {docs_loaded} documentos carregados")
            
            # Executa teste de performance
            performance_comparison_test()
        else:
            print("‚ö†Ô∏è Nenhum documento carregado")
    else:
        print("‚ùå Falha na inicializa√ß√£o do sistema otimizado")
        
def apply_advanced_search_fix(rag_system):
    """
    Aplica corre√ß√£o avan√ßada de busca para encontrar conte√∫do real dos documentos
    """
    print("üîß Aplicando corre√ß√£o avan√ßada de busca...")
    
    try:
        # 1. RECONFIGURA√á√ÉO COMPLETA PARA ENCONTRAR CONTE√öDO
        if hasattr(rag_system, 'config'):
            rag_system.config.min_similarity_score = 0.001  # ‚ö° MUITO mais permissivo
            rag_system.config.top_k = 8  # ‚ö° Mais documentos
            
        # 2. SISTEMA DE BUSCA H√çBRIDA AVAN√áADA
        def advanced_hybrid_search(query: str, top_k: int = 8) -> List:
            """Sistema de busca h√≠brida que garante encontrar conte√∫do relevante"""
            if not rag_system.vector_store:
                return []
            
            print(f"üîç Busca h√≠brida para: '{query}'")
            
            # === ESTRAT√âGIA 1: BUSCA POR KEYWORDS ESPEC√çFICAS ===
            keyword_results = []
            
            # Keywords espec√≠ficas para argumentos jur√≠dicos
            legal_keywords = {
                'argumento': ['argumento', 'argumenta', 'sustenta', 'defesa', 'alega'],
                'defesa': ['defesa', 'raz√µes de defesa', 'contradit√≥rio', 'ampla defesa', 'cerceamento'],
                'motivacao': ['motiva√ß√£o', 'fundamento', 'raz√£o', 'motivo', 'objetivo'],
                'pad': ['PAD', 'processo administrativo', 'disciplinar', 'demiss√£o'],
                'lei': ['lei municipal', 'lei complementar', 'art.', 'artigo'],
                'processo': ['processo', 'agravo', 'recurso', 'a√ß√£o'],
                'tutela': ['tutela', 'liminar', 'urg√™ncia', 'antecipa√ß√£o']
            }
            
            # Identifica tipo de busca e keywords relevantes
            query_lower = query.lower()
            relevant_keywords = []
            
            for categoria, keywords in legal_keywords.items():
                for keyword in keywords:
                    if keyword in query_lower:
                        relevant_keywords.extend(keywords)
            
            # Se n√£o encontrou keywords espec√≠ficas, usa palavras da query
            if not relevant_keywords:
                relevant_keywords = [word for word in query_lower.split() if len(word) > 3]
            
            print(f"üìù Keywords relevantes: {relevant_keywords}")
            
            # Busca por keywords nos documentos
            for i, doc in enumerate(rag_system.documents or []):
                content_lower = doc.page_content.lower()
                score = 0
                matches = []
                
                for keyword in relevant_keywords:
                    count = content_lower.count(keyword.lower())
                    if count > 0:
                        score += count * (len(keyword) / 10)  # Score ponderado por tamanho
                        matches.append(f"{keyword}({count})")
                
                if score > 0:
                    # Cria documento com score de keyword
                    doc_copy = type(doc)(
                        page_content=doc.page_content,
                        metadata={
                            **doc.metadata,
                            "similarity_score": min(score / 10, 1.0),  # Normaliza score
                            "search_type": "keyword",
                            "matches": matches
                        }
                    )
                    keyword_results.append((score, doc_copy))
            
            # Ordena por score e pega os melhores
            keyword_results.sort(key=lambda x: x[0], reverse=True)
            keyword_docs = [doc for _, doc in keyword_results[:top_k]]
            
            print(f"üéØ Busca por keywords encontrou: {len(keyword_docs)} documentos")
            
            # === ESTRAT√âGIA 2: BUSCA SEM√ÇNTICA MUITO PERMISSIVA ===
            semantic_results = []
            try:
                # Busca sem√¢ntica com threshold m√≠nimo
                semantic_results = rag_system.vector_store.similarity_search(
                    query, k=top_k, min_score=0.001  # Muito permissivo
                )
                print(f"üß† Busca sem√¢ntica encontrou: {len(semantic_results)} documentos")
            except Exception as e:
                print(f"‚ö†Ô∏è Busca sem√¢ntica falhou: {e}")
            
            # === ESTRAT√âGIA 3: BUSCA POR N√öMERO DE PROCESSO ===
            process_results = []
            process_pattern = r'\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4}'
            process_match = re.search(process_pattern, query)
            
            if process_match:
                process_number = process_match.group()
                print(f"üî¢ Buscando processo espec√≠fico: {process_number}")
                
                for doc in rag_system.documents or []:
                    if process_number in doc.page_content or process_number in str(doc.metadata):
                        doc_copy = type(doc)(
                            page_content=doc.page_content,
                            metadata={
                                **doc.metadata,
                                "similarity_score": 0.9,
                                "search_type": "process_number"
                            }
                        )
                        process_results.append(doc_copy)
                
                print(f"üî¢ Busca por processo encontrou: {len(process_results)} documentos")
            
            # === ESTRAT√âGIA 4: BUSCA POR CONTE√öDO SUBSTANTIVO ===
            substantial_results = []
            
            # Identifica documentos com conte√∫do substantivo (n√£o apenas metadados)
            for doc in rag_system.documents or []:
                content = doc.page_content
                
                # Score baseado em indicadores de conte√∫do substantivo
                substantial_score = 0
                
                # Frases que indicam conte√∫do jur√≠dico substantivo
                substantial_indicators = [
                    'sustenta', 'argumenta', 'alega', 'defende', 'contesta',
                    'fundamento', 'raz√£o', 'motivo', 'ementa', 'decis√£o',
                    'voto', 'ac√≥rd√£o', 'senten√ßa', 'despacho', 'parecer',
                    'lei municipal', 'c√≥digo de processo', 'constitui√ß√£o',
                    'jurisprud√™ncia', 'precedente', 's√∫mula'
                ]
                
                for indicator in substantial_indicators:
                    if indicator in content.lower():
                        substantial_score += 1
                
                # Penaliza documentos que s√£o s√≥ metadados
                if len(content) < 200 or content.count(':') > content.count('.'):
                    substantial_score *= 0.3
                
                if substantial_score > 2:  # Threshold para conte√∫do substantivo
                    doc_copy = type(doc)(
                        page_content=doc.page_content,
                        metadata={
                            **doc.metadata,
                            "similarity_score": min(substantial_score / 10, 1.0),
                            "search_type": "substantial_content"
                        }
                    )
                    substantial_results.append(doc_copy)
            
            print(f"üìö Busca substantiva encontrou: {len(substantial_results)} documentos")
            
            # === COMBINA√á√ÉO E RANKING FINAL ===
            all_results = []
            seen_content = set()
            
            # Prioriza por tipo de busca: processo > keyword > substantial > semantic
            for result_set, priority in [
                (process_results, 100),
                (keyword_docs, 80),
                (substantial_results, 60),
                (semantic_results, 40)
            ]:
                for doc in result_set:
                    # Evita duplicatas por conte√∫do
                    content_hash = hash(doc.page_content[:200])
                    if content_hash not in seen_content:
                        seen_content.add(content_hash)
                        
                        # Ajusta score com prioridade
                        current_score = doc.metadata.get("similarity_score", 0)
                        final_score = (current_score * priority) / 100
                        
                        doc.metadata["final_score"] = final_score
                        doc.metadata["priority"] = priority
                        all_results.append(doc)
            
            # Ordena por score final e retorna top_k
            all_results.sort(key=lambda x: x.metadata.get("final_score", 0), reverse=True)
            final_results = all_results[:top_k]
            
            print(f"‚úÖ Busca h√≠brida final: {len(final_results)} documentos selecionados")
            
            # Debug: mostra o que foi encontrado
            for i, doc in enumerate(final_results[:3], 1):
                search_type = doc.metadata.get("search_type", "unknown")
                score = doc.metadata.get("final_score", 0)
                preview = doc.page_content[:100].replace('\n', ' ')
                print(f"   {i}. {search_type} (score: {score:.3f}): {preview}...")
            
            return final_results
        
        # 3. SUBSTITUI O M√âTODO DE BUSCA
        rag_system._search_documents_optimized = lambda query, top_k: advanced_hybrid_search(query, top_k)
        
        # 4. M√âTODO DE CONTEXTO INTELIGENTE QUE PRIORIZA CONTE√öDO SUBSTANTIVO
        def create_intelligent_context(docs):
            """Cria contexto priorizando conte√∫do substantivo"""
            if not docs:
                return ""
            
            context_parts = []
            total_length = 0
            max_length = 20000  # Aumentado ainda mais
            
            # Ordena documentos por qualidade de conte√∫do
            docs_sorted = sorted(docs, key=lambda d: len(d.page_content), reverse=True)
            
            for i, doc in enumerate(docs_sorted, 1):
                content = doc.page_content.strip()
                
                # Metadados importantes
                metadata_info = []
                search_type = doc.metadata.get("search_type", "embedding")
                score = doc.metadata.get("final_score", doc.metadata.get("similarity_score", 0))
                
                for key in ['numero_processo', 'agravante', 'agravado', 'assuntos']:
                    if key in doc.metadata and doc.metadata[key]:
                        metadata_info.append(f"{key}: {doc.metadata[key]}")
                
                # Informa√ß√µes de debug da busca
                matches = doc.metadata.get("matches", [])
                debug_info = f"Busca: {search_type}, Score: {score:.3f}"
                if matches:
                    debug_info += f", Matches: {matches}"
                
                # Monta documento completo
                doc_text = f"DOCUMENTO {i} ({debug_info})"
                if metadata_info:
                    doc_text += f"\nMetadados: {', '.join(metadata_info)}"
                doc_text += f"\nConte√∫do:\n{content}"
                
                # Controle de tamanho total
                if total_length + len(doc_text) > max_length:
                    remaining = max_length - total_length - 500
                    if remaining > 1000:
                        doc_text = f"DOCUMENTO {i} (PARCIAL - {debug_info}):\n{content[:remaining]}..."
                        context_parts.append(doc_text)
                    break
                
                context_parts.append(doc_text)
                total_length += len(doc_text)
            
            return "\n\n" + "="*80 + "\n\n".join(context_parts)
        
        # Substitui m√©todo de contexto
        rag_system._create_optimized_context = lambda docs: create_intelligent_context(docs)
        
        # 5. PROMPT OTIMIZADO PARA EXTRAIR INFORMA√á√ïES ESPEC√çFICAS
        def create_extraction_prompt(question, context):
            return f"""Voc√™ √© um assistente jur√≠dico especializado. Analise os documentos fornecidos e responda de forma DETALHADA e COMPLETA.

INSTRU√á√ïES IMPORTANTES:
1. Use TODAS as informa√ß√µes relevantes dos documentos
2. Para argumentos de defesa: extraia alega√ß√µes, sustenta√ß√µes, argumenta√ß√µes espec√≠ficas
3. Para motiva√ß√µes: identifique o tipo de a√ß√£o, objeto, fundamentos jur√≠dicos
4. Cite trechos espec√≠ficos dos documentos quando relevante
5. Se houver informa√ß√µes contradit√≥rias, mencione todas as vers√µes
6. N√ÉO se limite apenas aos metadados - use o CONTE√öDO COMPLETO

DOCUMENTOS ANALISADOS:
{context}

PERGUNTA: {question}

RESPOSTA DETALHADA E FUNDAMENTADA:"""
        
        # Substitui m√©todo de prompt
        rag_system._create_optimized_prompt = create_extraction_prompt
        
        print("‚úÖ Corre√ß√£o avan√ßada de busca aplicada!")
        print("üéØ Melhorias implementadas:")
        print("  - Busca h√≠brida: keywords + sem√¢ntica + processo + conte√∫do")
        print("  - Threshold reduzido: 0.05 ‚Üí 0.001 (muito mais permissivo)")
        print("  - Prioriza√ß√£o de conte√∫do substantivo vs metadados")
        print("  - Sistema de ranking inteligente")
        print("  - Contexto expandido: 15000 ‚Üí 20000 chars")
        print("  - Prompt otimizado para extra√ß√£o espec√≠fica")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro na corre√ß√£o avan√ßada: {e}")
        return False

def test_advanced_search():
    """Testa o sistema de busca avan√ßada"""
    print("\nüß™ TESTANDO SISTEMA DE BUSCA AVAN√áADA...")
    
    if not optimized_rag_system:
        print("‚ùå Sistema n√£o inicializado")
        return
    
    # Testes espec√≠ficos que estavam falhando
    test_cases = [
        "Qual foi o argumento da defesa no processo 1002436-58.2025.8.11.0000?",
        "Qual a motiva√ß√£o do processo 1002436-58.2025.8.11.0000?",
        "O que a agravante alega sobre cerceamento de defesa?",
        "Quais s√£o os fundamentos da demiss√£o da servidora?",
        "Qual foi a decis√£o do tribunal sobre o PAD?"
    ]
    
    for i, question in enumerate(test_cases, 1):
        print(f"\n--- TESTE {i} ---")
        print(f"Pergunta: {question}")
        
        start_time = time.time()
        result = optimized_rag_system.query(question)
        elapsed = time.time() - start_time
        
        if "error" not in result:
            answer = result.get("answer", "")
            docs_found = result.get("documents_found", 0)
            
            print(f"‚úÖ Resposta obtida em {elapsed:.2f}s")
            print(f"üìä Documentos encontrados: {docs_found}")
            print(f"üìè Tamanho da resposta: {len(answer)} caracteres")
            
            # Verifica se encontrou conte√∫do substantivo
            if len(answer) > 200 and not answer.startswith("N√£o √© poss√≠vel"):
                print("üéâ SUCESSO: Conte√∫do substantivo encontrado!")
            else:
                print("‚ö†Ô∏è Ainda limitado aos metadados")
            
            print(f"üìù Pr√©via: {answer[:200]}...")
            
        else:
            print(f"‚ùå Erro: {result['error']}")

# =================================================================================
# INTERFACE SIMPLIFICADA
# =================================================================================

def fix_search_and_test():
    """Aplica corre√ß√£o e testa imediatamente"""
    print("üöÄ APLICANDO CORRE√á√ÉO AVAN√áADA DE BUSCA...")
    
    # Aplica a corre√ß√£o
    if apply_advanced_search_fix(optimized_rag_system):
        print("‚úÖ Corre√ß√£o aplicada com sucesso!")
        
        # Testa imediatamente
        test_advanced_search()
        
        print("\nüéØ SISTEMA PRONTO PARA USO!")
        print("Use: result = optimized_rag_system.query('sua pergunta')")
    else:
        print("‚ùå Falha na aplica√ß√£o da corre√ß√£o")

print("\n" + "="*80)
print("üîç CORRE√á√ÉO AVAN√áADA DE BUSCA DISPON√çVEL!")
print("="*80)
print("Para aplicar e testar:")
print("fix_search_and_test()")
print()
print("Para aplicar apenas a corre√ß√£o:")
print("apply_advanced_search_fix(optimized_rag_system)")
print("="*80)

def test_specific_questions():
    """Testa perguntas espec√≠ficas que estavam falhando"""
    if not optimized_rag_system:
        print("‚ùå Sistema n√£o inicializado")
        return
    
    questions = [
        "Qual foi o argumento da defesa no processo 1002436-58.2025.8.11.0000?",
        "Qual a motiva√ß√£o do processo 1002436-58.2025.8.11.0000?",
        "O que a agravante alega sobre cerceamento de defesa?",
        "Resuma o que trata o processo 1002436-58.2025.8.11.0000"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"TESTE {i}: {question}")
        print('='*60)
        
        result = optimized_rag_system.query(question)
        
        if "error" not in result:
            answer = result.get("answer", "")
            docs = result.get("documents_found", 0)
            
            print(f"üìä Docs encontrados: {docs}")
            print(f"üìè Tamanho resposta: {len(answer)} chars")
            print(f"üìù Resposta:\n{answer}")
            
            if len(answer) > 300 and "n√£o √© poss√≠vel" not in answer.lower():
                print("üéâ SUCESSO - Conte√∫do substantivo encontrado!")
            else:
                print("‚ö†Ô∏è Resposta ainda limitada")
        else:
            print(f"‚ùå Erro: {result['error']}")

print("\n" + "="*80)
print("üîß CORRE√á√ïES DIRETAS PARA SEU ARQUIVO")
print("="*80)
print("1. Substitua as se√ß√µes numeradas acima no seu arquivo")
print("2. Execute: init_optimized_rag() para inicializar")
print("3. Carregue dados: optimized_rag_system.load_documents_from_directory()")
print("4. Teste: test_specific_questions()")
print("="*80)