#!/usr/bin/env python3
"""
ğŸ¦™ Setup RÃ¡pido para Llama 3.1:8B + RAG Adaptativo
Configura tudo automaticamente para usar Llama 3.1:8B
"""

import os
import sys
import time
import subprocess
import requests
import json
from datetime import datetime

class Llama31Setup:
    """Setup automÃ¡tico para Llama 3.1:8B"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "llama3.1:8b"
        self.model_size = "4.7GB"
        
    def print_header(self):
        """Mostra header do setup"""
        print("ğŸ¦™ SETUP LLAMA 3.1:8B + RAG ADAPTATIVO")
        print("=" * 45)
        print(f"Modelo: {self.model_name}")
        print(f"Tamanho: {self.model_size}")
        print(f"Qualidade: â­â­â­â­ (Equilibrado)")
        print(f"Velocidade: âš¡âš¡âš¡ (Boa)")
        print("=" * 45)
    
    def check_python_deps(self) -> bool:
        """Verifica dependÃªncias Python"""
        print("\nğŸ“¦ Verificando dependÃªncias Python...")
        
        required = [
            "requests", "numpy", "pandas", "sklearn", 
            "sentence_transformers", "torch"
        ]
        
        missing = []
        for pkg in required:
            try:
                __import__(pkg.replace("-", "_"))
                print(f"   âœ… {pkg}")
            except ImportError:
                missing.append(pkg)
                print(f"   âŒ {pkg}")
        
        if missing:
            print(f"\nğŸ“¥ Instalando {len(missing)} dependÃªncia(s)...")
            try:
                cmd = [sys.executable, "-m", "pip", "install"] + missing
                subprocess.run(cmd, check=True)
                print("âœ… DependÃªncias instaladas!")
                return True
            except subprocess.CalledProcessError:
                print("âŒ Erro na instalaÃ§Ã£o. Tente manualmente:")
                print(f"   pip install {' '.join(missing)}")
                return False
        
        print("âœ… Todas as dependÃªncias estÃ£o instaladas!")
        return True
    
    def check_ollama_installed(self) -> bool:
        """Verifica se Ollama estÃ¡ instalado"""
        print("\nğŸ” Verificando Ollama...")
        
        try:
            result = subprocess.run(
                ["ollama", "--version"], 
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                version = result.stdout.strip()
                print(f"âœ… Ollama instalado: {version}")
                return True
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass
        
        print("âŒ Ollama nÃ£o encontrado")
        return False
    
    def install_ollama_instructions(self):
        """Mostra instruÃ§Ãµes para instalar Ollama"""
        print("\nğŸ“¥ COMO INSTALAR OLLAMA:")
        print("-" * 30)
        print("1. Visite: https://ollama.ai")
        print("2. Baixe o instalador para seu sistema")
        print("3. Execute o instalador")
        print("4. Reinicie o terminal")
        print("5. Execute este script novamente")
        print("\nğŸ’¡ Comandos alternativos:")
        print("   Linux: curl -fsSL https://ollama.ai/install.sh | sh")
        print("   macOS: brew install ollama")
        print("   Windows: Baixe o .exe do site")
    
    def check_ollama_running(self) -> bool:
        """Verifica se Ollama estÃ¡ rodando"""
        print("\nğŸ”Œ Verificando servidor Ollama...")
        
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… Ollama estÃ¡ rodando!")
                return True
        except requests.RequestException:
            pass
        
        print("âŒ Servidor Ollama nÃ£o estÃ¡ rodando")
        return False
    
    def start_ollama(self) -> bool:
        """Tenta iniciar Ollama"""
        print("\nğŸš€ Iniciando servidor Ollama...")
        
        try:
            # Tenta iniciar em background
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            
            # Aguarda alguns segundos
            print("â³ Aguardando servidor inicializar...")
            for i in range(15):
                time.sleep(1)
                try:
                    response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
                    if response.status_code == 200:
                        print("âœ… Servidor iniciado com sucesso!")
                        return True
                except:
                    pass
                print(f"   {i+1}/15...")
            
            print("âš ï¸ Servidor demorou para iniciar")
            print("ğŸ’¡ Tente manualmente: ollama serve")
            return False
            
        except Exception as e:
            print(f"âŒ Erro ao iniciar: {e}")
            return False
    
    def check_model_installed(self) -> bool:
        """Verifica se Llama 3.1:8B estÃ¡ instalado"""
        print(f"\nğŸ” Verificando {self.model_name}...")
        
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model_name in model_names:
                    print(f"âœ… {self.model_name} jÃ¡ instalado!")
                    return True
                
                if model_names:
                    print(f"ğŸ“‹ Modelos disponÃ­veis: {', '.join(model_names)}")
        except Exception as e:
            print(f"âŒ Erro ao verificar modelos: {e}")
        
        print(f"âŒ {self.model_name} nÃ£o encontrado")
        return False
    
    def install_llama31(self) -> bool:
        """Instala Llama 3.1:8B"""
        print(f"\nğŸ“¥ Baixando {self.model_name} ({self.model_size})...")
        print("â³ Isso pode demorar alguns minutos dependendo da sua conexÃ£o...")
        
        try:
            # Usa ollama CLI para mostrar progresso
            process = subprocess.Popen(
                ["ollama", "pull", self.model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True
            )
            
            # Mostra progresso
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # Filtra linhas de progresso
                    if any(word in output.lower() for word in ['pulling', 'downloading', 'verifying']):
                        print(f"   {output.strip()}")
            
            if process.returncode == 0:
                print(f"âœ… {self.model_name} instalado com sucesso!")
                return True
            else:
                print(f"âŒ Erro no download")
                return False
                
        except Exception as e:
            print(f"âŒ Erro durante instalaÃ§Ã£o: {e}")
            return False
    
    def test_llama31(self) -> bool:
        """Testa Llama 3.1:8B"""
        print(f"\nğŸ§ª Testando {self.model_name}...")
        
        test_prompt = "Responda apenas: 'Llama 3.1 funcionando!' em portuguÃªs"
        
        try:
            payload = {
                "model": self.model_name,
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 20
                }
            }
            
            print("â³ Gerando resposta de teste...")
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json().get('response', '').strip()
                processing_time = end_time - start_time
                
                print(f"âœ… Teste bem-sucedido!")
                print(f"   Resposta: {result}")
                print(f"   Tempo: {processing_time:.2f}s")
                
                if "llama" in result.lower() or "funcionando" in result.lower():
                    return True
                else:
                    print("âš ï¸ Resposta inesperada, mas modelo funciona")
                    return True
            else:
                print(f"âŒ Erro HTTP: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Erro no teste: {e}")
            return False
    
    def create_example_script(self):
        """Cria script de exemplo"""
        example_code = '''#!/usr/bin/env python3
"""
ğŸ¦™ Exemplo de uso do RAG Adaptativo com Llama 3.1:8B
"""

from adaptive_rag_llama31 import GMVAdaptiveRAGLlama31

def main():
    print("ğŸ¦™ EXEMPLO RAG + LLAMA 3.1:8B")
    print("=" * 35)
    
    # Inicializar RAG
    rag = GMVAdaptiveRAGLlama31()
    
    # Testar conexÃ£o com Llama
    print("\\nğŸ”Œ Testando Llama 3.1:8B...")
    test_results = rag.test_llama_connection()
    
    for query, result in test_results.items():
        if result.get("success"):
            print(f"   âœ… {query}: {result['response']}")
        else:
            print(f"   âŒ {query}: {result.get('error', 'Falhou')}")
    
    # Carregar dados (ajuste os caminhos)
    print("\\nğŸ“ Carregando dados...")
    try:
        success = rag.initialize(
            triagem_path="data/triagem.md",
            pasta_destino="data/processos/",
            pasta_dat="data/dat/"
        )
        
        if success:
            print("âœ… Dados carregados!")
            
            # EstatÃ­sticas
            stats = rag.get_statistics()
            print(f"ğŸ“Š {stats['total_documents']} documentos, {stats['total_chunks']} chunks")
            
            # Exemplo de consultas
            test_queries = [
                "Quais processos envolvem lavagem de dinheiro?",
                "Quantos processos estÃ£o em investigaÃ§Ã£o?",
                "Compare os tipos de crimes mais frequentes",
                "Explique o contexto dos processos suspeitos"
            ]
            
            print("\\nğŸ” Testando consultas...")
            for query in test_queries:
                print(f"\\nğŸ“ {query}")
                try:
                    result = rag.query(query)
                    print(f"   ğŸ¯ EstratÃ©gia: {result.strategy_used}")
                    print(f"   ğŸ“Š ConfianÃ§a: {result.confidence_score:.3f}")
                    print(f"   â±ï¸ Tempo: {result.processing_time:.2f}s")
                    print(f"   ğŸ’¬ Resposta: {result.response[:200]}...")
                except Exception as e:
                    print(f"   âŒ Erro: {e}")
        
        else:
            print("âš ï¸ Falha ao carregar dados - verifique os caminhos")
            
    except Exception as e:
        print(f"âŒ Erro: {e}")
        print("ğŸ’¡ Ajuste os caminhos dos arquivos no cÃ³digo")

if __name__ == "__main__":
    main()
'''
        
        try:
            with open("exemplo_llama31.py", "w", encoding="utf-8") as f:
                f.write(example_code)
            print("ğŸ“„ Exemplo criado: exemplo_llama31.py")
        except Exception as e:
            print(f"âš ï¸ Erro ao criar exemplo: {e}")
    
    def show_next_steps(self):
        """Mostra prÃ³ximos passos"""
        print("\nğŸ¯ PRÃ“XIMOS PASSOS")
        print("=" * 20)
        print("1. Execute o exemplo: python exemplo_llama31.py")
        print("2. Ajuste os caminhos dos seus dados")
        print("3. FaÃ§a suas prÃ³prias consultas")
        print("4. Monitore performance e ajuste conforme necessÃ¡rio")
        print("\nğŸ’¡ DICAS:")
        print("- Llama 3.1:8B Ã© equilibrado (qualidade + velocidade)")
        print("- Ideal para produÃ§Ã£o com hardware moderno")
        print("- Funciona bem em portuguÃªs brasileiro")
        print("- Use queries especÃ­ficas para melhores resultados")
    
    def run_setup(self):
        """Executa setup completo"""
        self.print_header()
        
        # 1. Verificar Python
        if not self.check_python_deps():
            print("âŒ Setup abortado - dependÃªncias Python")
            return False
        
        # 2. Verificar Ollama
        if not self.check_ollama_installed():
            self.install_ollama_instructions()
            return False
        
        # 3. Verificar servidor
        if not self.check_ollama_running():
            if not self.start_ollama():
                print("âŒ NÃ£o foi possÃ­vel iniciar Ollama")
                print("ğŸ’¡ Tente manualmente: ollama serve")
                return False
        
        # 4. Verificar modelo
        if not self.check_model_installed():
            if not self.install_llama31():
                print("âŒ Falha ao instalar Llama 3.1:8B")
                return False
        
        # 5. Testar modelo
        if not self.test_llama31():
            print("âŒ Llama 3.1:8B nÃ£o estÃ¡ funcionando")
            return False
        
        # 6. Criar exemplo
        self.create_example_script()
        
        # 7. Finalizar
        print("\nğŸ‰ SETUP CONCLUÃDO!")
        print("âœ… Llama 3.1:8B pronto para uso!")
        
        self.show_next_steps()
        
        return True

def main():
    """FunÃ§Ã£o principal"""
    try:
        setup = Llama31Setup()
        setup.run_setup()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Setup cancelado")
    except Exception as e:
        print(f"\nâŒ Erro inesperado: {e}")

if __name__ == "__main__":
    main()